{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scripts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col, when, unix_timestamp, hour, to_date, row_number, trim\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Window\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspark_session_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_spark_session\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mapp_constants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogger_impl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_logger, flush_logs_to_minio\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scripts'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, unix_timestamp, hour, to_date, row_number, trim\n",
    "from pyspark.sql.window import Window\n",
    "from spark_session_generator import create_spark_session\n",
    "from app_constants import *\n",
    "from logger_impl import set_logger, flush_logs_to_minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_crm_cust_info(spark):\n",
    "\n",
    "    try:\n",
    "        TABLE_NAME = \"crm_cust_info\"\n",
    "        logger_details = set_logger(SILVER_SCHEMA, TABLE_NAME)\n",
    "        logger = logger_details[0]\n",
    "        log_buffer = logger_details[1]\n",
    "        df = spark.table(f\"{ICEBERG_CATALOG}.{BRONZE_SCHEMA}.{TABLE_NAME}\")\n",
    "        logger.info(f\"Loaded table {TABLE_NAME} from {BRONZE_SCHEMA} layer\")\n",
    "\n",
    "        window_spec = Window.partitionBy(\"cst_id\").orderBy(df[\"cst_create_date\"].desc())\n",
    "        df_with_row_num = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "        ranked_cust_info_df = df_with_row_num.filter((col(\"row_num\") == 1) & (col(\"cst_id\").isNotNull())).drop(\"row_num\")\n",
    "        ranked_cust_info_df.createOrReplaceTempView(\"ranked_crm_cust_info\")\n",
    "        logger.info(\"Filtered latest records for each customer\")\n",
    "\n",
    "        silver_cust_info_df = spark.sql(\"\"\"SELECT cst_id,\n",
    "                                        cst_key,\n",
    "                                        TRIM(cst_firstname) AS cst_firstname,\n",
    "                                        TRIM(cst_lastname) AS cst_lastname,\n",
    "                                        CASE \n",
    "                                            WHEN UPPER(cst_marital_status) = 'S' THEN 'Single'\n",
    "                                            WHEN UPPER(cst_marital_status) = 'M' THEN 'Married'\n",
    "                                            ELSE 'n/a'\n",
    "                                        END cst_marital_status,\n",
    "                                        CASE \n",
    "                                            WHEN UPPER(cst_gndr) = 'F' THEN 'Female'\n",
    "                                            WHEN UPPER(cst_gndr) = 'M' THEN 'Male'\n",
    "                                            ELSE 'n/a'\n",
    "                                        END cst_gndr,\n",
    "                                        cst_create_date,\n",
    "                                        CURRENT_TIMESTAMP() AS dwh_create_date\n",
    "                                        FROM ranked_crm_cust_info\n",
    "                                    \"\"\")\n",
    "        logger.info(f\"Transformed data for {SILVER_SCHEMA} layer\")\n",
    "\n",
    "        silver_cust_info_df.write.format(TABLE_FORMAT) \\\n",
    "            .mode(INSERT_MODE) \\\n",
    "            .insertInto(f\"{ICEBERG_CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\")\n",
    "        \n",
    "        logger.info(f\"Data written to {SILVER_SCHEMA} layer table {TABLE_NAME}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load table {TABLE_NAME}: {e}\")\n",
    "        raise ValueError\n",
    "    \n",
    "    finally:\n",
    "        flush_logs_to_minio(logger, log_buffer, f\"{SILVER_SCHEMA}_logs/{TABLE_NAME}.log\")\n",
    "        logger.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_crm_prd_info(spark):\n",
    "\n",
    "    try:\n",
    "        TABLE_NAME = \"crm_prd_info\"\n",
    "        logger_details = set_logger(SILVER_SCHEMA, TABLE_NAME)\n",
    "        logger = logger_details[0]\n",
    "        log_buffer = logger_details[1]\n",
    "        logger.info(f\"Loading table {TABLE_NAME} from {BRONZE_SCHEMA} layer\")\n",
    "        silver_prd_info_df = spark.sql(f\"\"\"SELECT prd_id,\n",
    "                    REPLACE(SUBSTR(prd_key, 1, 5),'-', '_') AS cat_id,\n",
    "                    SUBSTR(prd_key, 7, LENGTH(prd_key)) AS prd_key,\n",
    "                    prd_nm,\n",
    "                    COALESCE(prd_cost, 0) AS prd_cost,\n",
    "                    CASE UPPER(TRIM(prd_line))\n",
    "                        WHEN 'M' THEN 'Mountain'\n",
    "                        WHEN 'R' THEN 'Road'\n",
    "                        WHEN 'S' THEN 'Other Sales'\n",
    "                        ELSE 'n/a'\n",
    "                    END \n",
    "                    AS prd_line,\n",
    "                    prd_start_dt,\n",
    "                    LEAD(prd_start_dt) OVER (PARTITION BY prd_key ORDER BY prd_start_dt) - 1 AS prd_end_dt_new,\n",
    "                    CURRENT_TIMESTAMP() AS dwh_create_date\n",
    "                FROM {ICEBERG_CATALOG}.{BRONZE_SCHEMA}.{TABLE_NAME}\n",
    "                \"\"\")\n",
    "        \n",
    "        logger.info(f\"Transformed data for {SILVER_SCHEMA} layer\")\n",
    "        \n",
    "        silver_prd_info_df.write.format(TABLE_FORMAT) \\\n",
    "            .mode(INSERT_MODE) \\\n",
    "            .insertInto(f\"{ICEBERG_CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\")\n",
    "        \n",
    "        logger.info(f\"Data written to {SILVER_SCHEMA} layer table {TABLE_NAME}\")\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load table {TABLE_NAME}: {e}\")\n",
    "        raise ValueError\n",
    "    \n",
    "    finally:\n",
    "        flush_logs_to_minio(logger, log_buffer, f\"{SILVER_SCHEMA}_logs/{TABLE_NAME}.log\")\n",
    "        logger.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_crm_sales_details(spark):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        TABLE_NAME = \"crm_sales_details\"\n",
    "        logger_details = set_logger(SILVER_SCHEMA, TABLE_NAME)\n",
    "        logger = logger_details[0]\n",
    "        log_buffer = logger_details[1]\n",
    "\n",
    "        silver_sales_details_df = spark.sql(f\"\"\"SELECT sls_ord_num,\n",
    "                        sls_prd_key,\n",
    "                        sls_cust_id,\n",
    "                        CASE WHEN sls_order_dt = 0 OR LENGTH(sls_order_dt) != 8\n",
    "                             THEN NULL\n",
    "                             ELSE CAST(TO_DATE(CAST(sls_order_dt AS STRING), 'yyyyMMdd') AS DATE) \n",
    "                        END AS sls_order_dt,\n",
    "                        CASE WHEN sls_ship_dt = 0 OR LENGTH(sls_ship_dt) != 8\n",
    "                             THEN NULL\n",
    "                             ELSE CAST(TO_DATE(CAST(sls_ship_dt AS STRING), 'yyyyMMdd') AS DATE) \n",
    "                        END AS sls_ship_dt,\n",
    "                        CASE WHEN sls_due_dt = 0 OR LENGTH(sls_due_dt) != 8\n",
    "                             THEN NULL\n",
    "                             ELSE CAST(TO_DATE(CAST(sls_due_dt AS STRING), 'yyyyMMdd') AS DATE) \n",
    "                        END AS sls_due_dt,\n",
    "                        CASE \n",
    "                            WHEN sls_sales IS NULL OR sls_sales <= 0 OR sls_sales != sls_quantity * ABS(sls_price)\n",
    "                             THEN sls_quantity * ABS(sls_price)\n",
    "                             ELSE sls_sales\n",
    "                            END AS sls_sales,\n",
    "                        sls_quantity,\n",
    "                        CASE \n",
    "                            WHEN sls_price IS NULL OR sls_price <= 0 \n",
    "                             THEN sls_sales / NULLIF(sls_quantity, 0)\n",
    "                             ELSE sls_price\n",
    "                        END AS sls_price,\n",
    "                        CURRENT_TIMESTAMP() AS dwh_create_date\n",
    "                    FROM {ICEBERG_CATALOG}.{BRONZE_SCHEMA}.{TABLE_NAME}; \n",
    "                  \"\"\")\n",
    "        \n",
    "        logger.info(f\"Transformed data for {SILVER_SCHEMA} layer\")\n",
    "\n",
    "        # df.show(5)\n",
    "        \n",
    "        silver_sales_details_df.write.format(TABLE_FORMAT) \\\n",
    "            .mode(INSERT_MODE) \\\n",
    "            .insertInto(f\"{ICEBERG_CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\")\n",
    "        \n",
    "        logger.info(f\"Data written to {SILVER_SCHEMA} layer table {TABLE_NAME}\")\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load table {TABLE_NAME}: {e}\")\n",
    "        raise ValueError\n",
    "    \n",
    "    finally:\n",
    "        flush_logs_to_minio(logger, log_buffer, f\"{SILVER_SCHEMA}_logs/{TABLE_NAME}.log\")\n",
    "        logger.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_erp_cust_az12(spark):\n",
    "    \n",
    "    try:\n",
    "\n",
    "        TABLE_NAME = \"erp_cust_az12\"\n",
    "        logger_details = set_logger(SILVER_SCHEMA, TABLE_NAME)\n",
    "        logger = logger_details[0]\n",
    "        log_buffer = logger_details[1]\n",
    "\n",
    "        silver_cust_az12_df = spark.sql(f\"\"\"SELECT \n",
    "                             CASE \n",
    "                                WHEN cid like 'NAS%' THEN SUBSTR(cid, 4,LENGTH(cid))\n",
    "                                ELSE cid  \n",
    "                             END AS cst_id,\n",
    "                             CASE \n",
    "                                WHEN bdate > CURRENT_DATE() THEN NULL\n",
    "                                ELSE bdate\n",
    "                             END AS bdate,\n",
    "                             CASE \n",
    "                                WHEN UPPER(TRIM(gen)) IN ('F', 'FEMALE') THEN 'Female'\n",
    "                                WHEN UPPER(TRIM(gen)) IN ('M', 'MALE') THEN 'Male'\n",
    "                                ELSE 'n/a'\n",
    "                             END as gen,\n",
    "                             CURRENT_TIMESTAMP() AS dwh_create_date\n",
    "                            FROM {ICEBERG_CATALOG}.{BRONZE_SCHEMA}.{TABLE_NAME}\n",
    "                  \"\"\")\n",
    "        \n",
    "        logger.info(f\"Transformed data for {SILVER_SCHEMA} layer\")\n",
    "\n",
    "        # df.show(5)\n",
    "        \n",
    "        silver_cust_az12_df.write.format(TABLE_FORMAT) \\\n",
    "            .mode(INSERT_MODE) \\\n",
    "            .insertInto(f\"{ICEBERG_CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\")\n",
    "        \n",
    "        logger.info(f\"Data written to {SILVER_SCHEMA} layer table {TABLE_NAME}\")\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load table {TABLE_NAME}: {e}\")\n",
    "        raise ValueError\n",
    "    \n",
    "    finally:\n",
    "        flush_logs_to_minio(logger, log_buffer, f\"{SILVER_SCHEMA}_logs/{TABLE_NAME}.log\")\n",
    "        logger.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_erp_loc_a101(spark):\n",
    "    \n",
    "    try:\n",
    "        TABLE_NAME = \"erp_loc_a101\"\n",
    "        logger_details = set_logger(SILVER_SCHEMA, TABLE_NAME)\n",
    "        logger = logger_details[0]\n",
    "        log_buffer = logger_details[1]\n",
    "\n",
    "        silver_loc_a101_df = spark.sql(f\"\"\"SELECT \n",
    "                             REPLACE(cid, '-','') as cid,\n",
    "                             CASE \n",
    "                                WHEN TRIM(cntry) = 'DE' THEN 'Germany'\n",
    "                                WHEN TRIM(cntry) IN ('US', 'USA') THEN 'United States'\n",
    "                                WHEN TRIM(cntry) = '' OR TRIM(cntry) IS NULL THEN 'n/a'\n",
    "                                ELSE TRIM(cntry)\n",
    "                             END AS cntry,\n",
    "                             CURRENT_TIMESTAMP() AS dwh_create_date\n",
    "                            FROM {ICEBERG_CATALOG}.{BRONZE_SCHEMA}.{TABLE_NAME}\n",
    "                  \"\"\")\n",
    "        \n",
    "        logger.info(f\"Transformed data for {SILVER_SCHEMA} layer\")\n",
    "        \n",
    "        silver_loc_a101_df.write.format(TABLE_FORMAT) \\\n",
    "            .mode(INSERT_MODE) \\\n",
    "            .insertInto(f\"{ICEBERG_CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\")\n",
    "        \n",
    "        logger.info(f\"Data written to {SILVER_SCHEMA} layer table {TABLE_NAME}\")\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load table {TABLE_NAME}: {e}\")\n",
    "        raise ValueError\n",
    "    \n",
    "    finally:\n",
    "        flush_logs_to_minio(logger, log_buffer, f\"{SILVER_SCHEMA}_logs/{TABLE_NAME}.log\")\n",
    "        logger.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_erp_px_cat_g1v2(spark):\n",
    "    \n",
    "    try:\n",
    "        TABLE_NAME = \"erp_px_cat_g1v2\"\n",
    "        logger_details = set_logger(SILVER_SCHEMA, TABLE_NAME)\n",
    "        logger = logger_details[0]\n",
    "        log_buffer = logger_details[1]\n",
    "\n",
    "        silver_erp_px_cat_g1v2_df = spark.sql(\"\"\"SELECT \n",
    "                             id,\n",
    "                             cat,\n",
    "                             subcat,\n",
    "                             maintenance,\n",
    "                             CURRENT_TIMESTAMP() AS dwh_create_date\n",
    "                            FROM erp_px_cat_g1v2\n",
    "                  \"\"\")\n",
    "        \n",
    "        logger.info(f\"Transformed data for {SILVER_SCHEMA} layer\")\n",
    "        \n",
    "        silver_erp_px_cat_g1v2_df.write.format(TABLE_FORMAT) \\\n",
    "            .mode(INSERT_MODE) \\\n",
    "            .insertInto(f\"{ICEBERG_CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\")\n",
    "        \n",
    "        logger.info(f\"Data written to {SILVER_SCHEMA} layer table {TABLE_NAME}\")\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load table {TABLE_NAME}: {e}\")\n",
    "        raise ValueError\n",
    "    \n",
    "    finally:\n",
    "        flush_logs_to_minio(logger, log_buffer, f\"{SILVER_SCHEMA}_logs/{TABLE_NAME}.log\")\n",
    "        logger.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    spark = create_spark_session()\n",
    "    \n",
    "    load_crm_cust_info(spark)\n",
    "    load_crm_prd_info(spark)\n",
    "    load_crm_sales_details(spark)\n",
    "    load_erp_cust_az12(spark)\n",
    "    load_erp_loc_a101(spark)\n",
    "    load_erp_px_cat_g1v2(spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
